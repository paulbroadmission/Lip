利普希茨连续
什么是利普希茨连续？

满足如下性质的任意连续函数 
 称为K-Lipschitz:


从导数上来看，可以看出满足K-Lipschitz的函数 
 在任意两点之间的连线斜率小于等于 
 ，那么也就是函数任意点的导数也小于等于 
 。

这个性质的函数在机器学习模型中是很受欢迎的，我们都希望能够训练一个稳健的模型。那么何为稳健？一般来说有两种含义：一是模型（系统）对本身参数扰动的稳定性，即模型的某些参数发生了变化，是否可以仍然对相同的输入保持近似的响应，即希望 
 ，其中响应偏移量可控（ 
 ），比如一个分类模型在某些参数轻微扰动后，仍然能够识别出猫这一类别，那么说明该模型具有稳定性。在动力学系统中，抗参数扰动一般还要求，系统在参数扰动后仍然能够恢复到 
 ，比如无人机的某个旋翼突然偏重偏轻，无人机仍然能够恢复平稳飞行。二是模型对输入扰动的稳定性，即模型在输入发生轻微扰动变化，对新输入的响应不变，即希望 
 ，然而目前很多的对抗性工作王晋东不在家：对抗鲁棒性简介表明模型还是很脆弱的，而后续的很多工作也围绕利普希茨约束展开。

利普希茨约束
从上面的分析我们得知，我们希望得到一个对输入扰动不敏感的模型，这样的模型会更加鲁棒。也就是说，当 
 很小时， 
 也尽可能的很小。当然，“尽可能”肯定不能作为量化指标，那么我们定义一个更具体的约束，即，存在一个常数 
 （它只与模型参数有关，与输入无关，记该常数为 
 ），使得下式恒成立


也就是说。模型被一个常数“线性”约束住了，这便是 
 约束了。

换言之，在这里我们认为满足 
 约束的模型才是一个好模型。并且对于具体的模型，我们希望估算出 
 的表达式，并且希望 
 越小越好，越小意味着它对输入扰动越不敏感，泛化性越好。

神经网络
我们考虑全连接网络的第 
 层，该层网络的权重为 
 ，偏置为 
 ，激活函数为 
 (这里用 
 是为了便于书写，一般我们用 
 ），那么对于输入 
 和 
 ，有


让 
 和 
 充分接近，可以让它们是值 
 的 
 -邻域中的任意两个数，即， 
 ，那么上式小于等于号左侧部分在 
 处的一阶泰勒展开并化简为：


其实在 
 非常小时，一阶泰勒展开精度就已经很高了，二阶展开处也不会超过两个 
 的误差，所以基本可以忽略高阶泰勒。

那么左侧中有三项，分别是 
，
，
 ，如果要使得上式成立，那么显然 
 不能无限大，至少要约束在小于某一个常数的范围。这就要求模型训练过程中要选择“导数有上下界”的激活函数。我们平时使用的激活函数sigmoid, tanh, Relu等都是符合这个要求的，特别是ReLu对输入的导数不超过1，是个很适用的激活函数。那么，假设我们选择了ReLu激活函数，那么上式的三项可以变为两项，即我们可以忽略激活函数导数项 
 了，多层的神经网络可以逐步递归分析，从而最终还是单层的神经网络问题，而 CNN、 RNN 等结构本质上还是特殊的全连接，所以照样可以用全连接的结果。因此，对于神经网络来说，问题变成了：


如果我们希望上式恒成立，那么这个常数如何得到呢？

L2正则化
是的，一个最容易推导出来的约束是L2正则化。上式很容易看出，我们只要约束


就能保证不等式成立，那么我们不显式地定义常数 
 ，只是把该项引入到模型的优化损失中即可，让模型在每次更新过程都让参数尽可能的小，不能随意地扩张，这就是我们经常遇到的L2正则化，这也解释了L2正则化背后的意义，


这揭示了 L2 正则化（也称为 weight decay）与 L 约束的联系，表明 L2 正则化能使得模型更好地满足 L 约束，从而降低模型对输入扰动的敏感性，增强模型的泛化性能。

矩阵范数
其实L2正则化仍然没有找到常数 
 的最优解，因为我们默认用了参数矩阵的F范数来表示L2正则化，其实还有一个更准确地矩阵范数，即谱范数，该范数的定义为


如果 
 是一个方阵，我们把 
 叫做 
 的“谱范数（Spectral Norm）”、“谱半径”等，在本文中即使参数矩阵不是方阵，我们也这么称呼。注意 
 和 
 都是指向量的范数，就是普通的向量模长。而左边的矩阵的范数我们本来没有明确定义的，但通过右边的向量模型的极限定义出来的，所以这类矩阵范数称为

“由向量范数诱导出来的矩阵范数”。

有了矩阵范数的概念之后，我们就有


Frobenius 范数
其实谱范数 
 比较难计算，那么我们先研究一个更加简单的范数： Frobenius 范数，简称 F 范数


其中 
 是参数矩阵的第p行第q列的元素。因此矩阵的F范数，等同于直接把矩阵当成一个向量，然后求向量的欧氏模长。

简单通过柯西不等式，我们就能证明


很明显 
 提供了 
 的一个上界. 也就是说，以理解为 
 是 
 中最准确的 C（所有满足此公式的 C 中最小的那个），但如果

不大关心精准度，直接可以取 
 ，也能使得公式成立，毕竟 F范数比较容易计算。

GAN 中的 K-Lipschitz 约束
weight clipping
由 Wasserstein GAN 提出。在利用 gradient descent 进行参数更新后，再对所有参数进行截断操作：

if w < -threshold:
    w = -threshold
elif w > threshold:
    w = threshold
else:
    w = identity(w)
其中 threshold是人为设定的阈值。注意， Weight cplipping 并无法保证 f 位于 1-Lipschitz，而只

能保证其是 K-Lipschitz 的（K 具体无法确定）。

Gradient penalty
由 Improved Training of Wasserstein GANs 提出。

理论支持：一个可微函数 f 是 1-Lipschitz 当且仅当它对所有的输入 x 均满足


在具体实现时，即在 Objective function 中添加如下正则项：


直面谱范数
虽然利普希茨约束应用广泛，但我们转了一圈，最终还是要直面谱范数如何求解的问题。

这部分我们来正式面对谱范数 
 ，这是线性代数的内容，比较理论化。事实上，谱范数 
 等于 
 的最大特征根（主特征根）的平方根，如果 
 是方阵，那么 
 等于 
 的最大的特征根绝对值。

这里提供一下证明的大概思路


假设 
 对角化为矩阵 
 ，即 
 ，其中 
 是 
 的第j大特征根，而且非负，而 U 是正交矩阵，由于正交矩阵与单位向量的积还是单位向量，

那么


得证。

幂迭代求解 谱范数 
在这里提供一个简单的求解谱范数的算法，成为幂迭代求解法。通过幂迭代法可以求出矩阵 
 的最大特征值，具体地，可以通过如下方式迭代


u的初始化可以是任意的，那么通过若干次迭代更新u后，此时的向量u就是矩阵 
 最大特征值 
 对应的单位特征值，那么最大特征值可以表示为


再次我给出简单的证明，令 
 ,其n个特征值从大到小依次为 
 ,它们对应的单位特征向量为 
 ,那么n维空间中的任意一个向量 
 可由这n个正交的特征向量来表示（即这n个特征向量构成空间的一组基），记为 
 ,那么


因为 
 是一个小于1的数，当k比较大时，其k次方约等于0，所以 
 在k比较大时，其在空间中的方向就对应着最大特征值对应特征向量的方向，而我们需要的只是这个“方向”，然后我们把 
 单位化，就得到了最大特征值的单位特征向量，即 
 ，然后最大特征值就是 
 ，证毕。

额外的思考：上述的方法可以求得最大特征值，那么是不是也可以求解最小特征值？再然后是不是可以求任意特征值？大家可以参考一下yxue3357：求特征值的迭代数值算法和Power Iteration - ML Wiki，上面的方法只需要把A的逆带进去就得到最小特征值。