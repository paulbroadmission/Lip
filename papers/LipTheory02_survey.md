# Lipschitz 連續性理論概述

## 什麼是 Lipschitz 連續？

滿足如下性質的任意連續函數 $f$ 稱為 K-Lipschitz：

$$\|f(x_1) - f(x_2)\| \leq K \|x_1 - x_2\|$$

從導數上來看，可以看出滿足 K-Lipschitz 的函數 $f$ 在任意兩點之間的連線斜率的絕對值小於等於 $K$。對於一元可微函數，這意味著 $|f'(x)| ≤ K$ 對所有 $x$ 成立。

這個性質的函數在機器學習模型中是很受歡迎的，我們都希望能夠訓練一個穩健的模型。那麼何為穩健？一般來說有兩種含義：

一是模型（系統）對本身參數擾動的穩定性，即模型的某些參數發生了變化，是否可以仍然對相同的輸入保持近似的響應，即希望 $\|f_{\theta_1}(x) - f_{\theta_2}(x)\| \leq \epsilon$，其中響應偏移量可控（$\epsilon$ 很小），比如一個分類模型在某些參數輕微擾動後，仍然能夠識別出貓這一類別，那麼說明該模型具有穩定性。在動力學系統中，抗參數擾動一般還要求，系統在參數擾動後仍然能夠恢復到平衡點，比如無人機的某個旋翼突然偏重偏輕，無人機仍然能夠恢復平穩飛行。

二是模型對輸入擾動的穩定性，即模型在輸入發生輕微擾動變化，對新輸入的響應不變，即希望 $\|f(x_1) - f(x_2)\| \leq \epsilon$，然而目前很多的對抗性工作表明模型還是很脆弱的，而後續的很多工作也圍繞 Lipschitz 約束展開。

## Lipschitz 約束

從上面的分析我們得知，我們希望得到一個對輸入擾動不敏感的模型，這樣的模型會更加穩健。也就是說，當 $\|x_1 - x_2\|$ 很小時，$\|f(x_1) - f(x_2)\|$ 也盡可能的很小。當然，「盡可能」肯定不能作為量化指標，那麼我們定義一個更具體的約束，即，存在一個常數 $K$（它只與模型參數有關，與輸入無關，記該常數為 Lipschitz 常數），使得下式恆成立：

$$\|f(x_1) - f(x_2)\| \leq K \|x_1 - x_2\|$$

也就是說，模型被一個常數「線性」約束住了，這便是 K-Lipschitz 約束了。

換言之，在這裡我們認為滿足 K-Lipschitz 約束的模型才是一個好模型。並且對於具體的模型，我們希望估算出 $K$ 的表達式，並且希望 $K$ 越小越好，越小意味著它對輸入擾動越不敏感，泛化性越好。

## 神經網路

我們考慮全連接網路的第 $l$ 層，該層網路的權重為 $W^{(l)}$，偏置為 $b^{(l)}$，激活函數為 $\sigma$ (這裡用 $\sigma$ 是為了便於書寫，一般我們用 $f$），那麼對於輸入 $x_1$ 和 $x_2$，有：

$$\|W^{(l)} \sigma(x_1) + b^{(l)} - W^{(l)} \sigma(x_2) - b^{(l)}\| = \|W^{(l)} (\sigma(x_1) - \sigma(x_2))\|$$

讓 $x_1$ 和 $x_2$ 充分接近，可以讓它們是值 $x$ 的 $\epsilon$-鄰域中的任意兩個數，即，$x_1, x_2 \in B(x, \epsilon)$，那麼上式小於等於號左側部分在 $x$ 處的一階泰勒展開並化簡為：

$$\|W^{(l)} (\sigma(x_1) - \sigma(x_2))\| \approx \|W^{(l)} \sigma'(x) (x_1 - x_2)\|$$

其實在 $\epsilon$ 非常小時，一階泰勒展開精度就已經很高了，二階展開處也不會超過兩個 $\epsilon$ 的誤差，所以基本可以忽略高階泰勒。

那麼左側中有三項，分別是 $W^{(l)}$、$\sigma'(x)$、$(x_1 - x_2)$，如果要使得上式成立，那麼顯然 $\sigma'(x)$ 不能無限大，至少要約束在小於某一個常數的範圍。這就要求模型訓練過程中要選擇「導數有上下界」的激活函數。我們平時使用的激活函數 sigmoid、tanh、ReLU 等都是符合這個要求的，特別是 ReLU 對輸入的導數不超過1，是個很適用的激活函數。那麼，假設我們選擇了 ReLU 激活函數，那麼上式的三項可以變為兩項，即我們可以忽略激活函數導數項 $\sigma'(x)$ 了，多層的神經網路可以逐步遞歸分析，從而最終還是單層的神經網路問題，而 CNN、RNN 等結構本質上還是特殊的全連接，所以照樣可以用全連接的結果。因此，對於神經網路來說，問題變成了：

$$\|W^{(l)} (x_1 - x_2)\| \leq K \|x_1 - x_2\|$$

如果我們希望上式恆成立，那麼這個常數如何得到呢？

## L2 正則化

是的，一個最容易推導出來的約束是 L2 正則化。上式很容易看出，我們只要約束：

$$\|W^{(l)}\|_F \leq K$$

就能保證不等式成立，那麼我們不顯式地定義常數 $K$，只是把該項引入到模型的優化損失中即可，讓模型在每次更新過程都讓參數盡可能的小，不能隨意地擴張，這就是我們經常遇到的 L2 正則化，這也解釋了 L2 正則化背後的意義：

$$\mathcal{L} = \mathcal{L}_{\text{original}} + \lambda \|W\|_F^2$$

這揭示了 L2 正則化（也稱為 weight decay）與 Lipschitz 約束的聯繫，表明 L2 正則化能使得模型更好地滿足 Lipschitz 約束，從而降低模型對輸入擾動的敏感性，增強模型的泛化性能。

## 矩陣範數

其實 L2 正則化仍然沒有找到常數 $K$ 的最優解，因為我們默認用了參數矩陣的 F 範數來表示 L2 正則化，其實還有一個更準確地矩陣範數，即譜範數，該範數的定義為：

$$\|A\|_2 = \max_{\|x\|_2=1} \|Ax\|_2$$

如果 $A$ 是一個方陣，我們把 $\|A\|_2$ 叫做 $A$ 的「譜範數（Spectral Norm）」。注意譜範數與譜半徑是不同的概念：譜半徑是特徵值絕對值的最大值，而譜範數總是等於 $\sigma_1(A)$（最大奇異值）。注意 $\|x\|_2$ 和 $\|Ax\|_2$ 都是指向量的範數，就是普通的向量模長。而左邊的矩陣的範數我們本來沒有明確定義的，但通過右邊的向量模型的極限定義出來的，所以這類矩陣範數稱為

「由向量範數誘導出來的矩陣範數」。

有了矩陣範數的概念之後，我們就有：

$$\|Ax\| \leq \|A\|_2 \|x\|$$

## Frobenius 範數

其實譜範數 $\|A\|_2$ 比較難計算，那麼我們先研究一個更加簡單的範數：Frobenius 範數，簡稱 F 範數：

$$\|A\|_F = \sqrt{\sum_{p,q} A_{pq}^2}$$

其中 $A_{pq}$ 是參數矩陣的第 p 行第 q 列的元素。因此矩陣的 F 範數，等同於直接把矩陣當成一個向量，然後求向量的歐氏模長。

簡單通過柯西不等式，我們就能證明：

$$\|A\|_2 \leq \|A\|_F$$

很明顯 $\|A\|_F$ 提供了 $\|A\|_2$ 的一個上界。也就是說，可以理解為 $\|A\|_2$ 是最準確的 C（所有滿足此公式的 C 中最小的那個），但如果不大關心精準度，直接可以取 $\|A\|_F$，也能使得公式成立，畢竟 F 範數比較容易計算。

## GAN 中的 K-Lipschitz 約束

### Weight Clipping

由 Wasserstein GAN 提出。在利用 gradient descent 進行參數更新後，再對所有參數進行截斷操作：

```python
if w < -threshold:
    w = -threshold
elif w > threshold:
    w = threshold
else:
    w = identity(w)
```

其中 threshold 是人為設定的閾值。注意，Weight clipping 並無法保證 f 位於 1-Lipschitz，而只能保證其是 K-Lipschitz 的（K 具體無法確定）。

### Gradient Penalty

由 Improved Training of Wasserstein GANs 提出。

理論支持：一個可微函數 f 是 1-Lipschitz 當且僅當它對所有的輸入 x 均滿足：

$$\|\nabla_x f(x)\|_2 \leq 1$$

在具體實現時，即在 Objective function 中添加如下正則項：

$$\mathcal{L}_{GP} = \lambda \mathbb{E}_{\hat{x} \sim P_{\hat{x}}} \left[ \left( \|\nabla_{\hat{x}} D(\hat{x})\|_2 - 1 \right)^2 \right]$$

## 直面譜範數

雖然 Lipschitz 約束應用廣泛，但我們轉了一圈，最終還是要直面譜範數如何求解的問題。

這部分我們來正式面對譜範數 $\|A\|_2$，這是線性代數的內容，比較理論化。事實上，譜範數 $\|A\|_2$ 等於 $A^T A$ 的最大特徵值的平方根，也就是 $A$ 的最大奇異值 $\sigma_1(A)$。注意：即使 $A$ 是方陣，$\|A\|_2$ 一般也不等於 $A$ 的最大特徵值絕對值，除非 $A$ 是正規矩陣（normal matrix）。

這裡提供一下證明的大概思路：

假設 $A^T A$ 對角化為矩陣 $U \Lambda U^T$，即 $A^T A = U \Lambda U^T$，其中 $\lambda_j$ 是 $A^T A$ 的第 j 大特徵根，而且非負，而 U 是正交矩陣，由於正交矩陣與單位向量的積還是單位向量，

那麼：

$$\|A\|_2^2 = \max_{\|x\|=1} \|Ax\|^2 = \max_{\|x\|=1} x^T A^T A x = \max_{\|x\|=1} x^T U \Lambda U^T x = \max_{\|y\|=1} y^T \Lambda y = \lambda_1$$

因此 $\|A\|_2 = \sqrt{\lambda_1}$。

得證。

## 幂迭代求解譜範數

在這裡提供一個簡單的求解譜範數的算法，稱為幂迭代求解法。通過幂迭代法可以求出矩陣 $A^T A$ 的最大特徵值，具體地，可以通過如下方式迭代：

$$u^{(k+1)} = \frac{A^T A u^{(k)}}{\|A^T A u^{(k)}\|}$$

u 的初始化可以是任意的，那麼通過若干次迭代更新 u 後，此時的向量 u 就是矩陣 $A^T A$ 最大特徵值 $\lambda_1$ 對應的單位特徵向量，那麼最大特徵值可以表示為：

$$\lambda_1 = u^T A^T A u$$

再次我給出簡單的證明，令 $M = A^T A$，其 n 個特徵值從大到小依次為 $\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_n$，它們對應的單位特徵向量為 $v_1, v_2, \ldots, v_n$，那麼 n 維空間中的任意一個向量 $u^{(0)}$ 可由這 n 個正交的特徵向量來表示（即這 n 個特徵向量構成空間的一組基），記為 $u^{(0)} = \sum_{i=1}^n c_i v_i$，那麼：

$$u^{(k)} = \frac{M^k u^{(0)}}{\|M^k u^{(0)}\|} = \frac{\sum_{i=1}^n c_i \lambda_i^k v_i}{\|\sum_{i=1}^n c_i \lambda_i^k v_i\|} = \frac{c_1 \lambda_1^k v_1 + \sum_{i=2}^n c_i \lambda_i^k v_i}{\|c_1 \lambda_1^k v_1 + \sum_{i=2}^n c_i \lambda_i^k v_i\|}$$

因為 $\frac{\lambda_i}{\lambda_1}$ 是一個小於1的數，當 k 比較大時，其 k 次方約等於0，所以 $u^{(k)}$ 在 k 比較大時，其在空間中的方向就對應著最大特徵值對應特徵向量的方向，而我們需要的只是這個「方向」，然後我們把 $u^{(k)}$ 單位化，就得到了最大特徵值的單位特徵向量，即 $v_1$，然後最大特徵值就是 $\lambda_1 = u^T M u$，證畢。

額外的思考：上述的方法可以求得最大特徵值，那麼是不是也可以求解最小特徵值？再然後是不是可以求任意特徵值？上面的方法只需要把 A 的逆帶進去就得到最小特徵值。